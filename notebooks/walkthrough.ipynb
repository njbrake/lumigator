{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3373f2c7-da8e-49eb-8300-25ae4a00fdaa",
   "metadata": {},
   "source": [
    "# Lumigator from [Mozilla.ai](https://www.mozilla.ai/) üêä ü¶ä"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ba77ea-aae4-4c55-ad18-1b665836532c",
   "metadata": {},
   "source": [
    "This Notebook will guide you through:\n",
    "\n",
    "+ What Jupyter Notebooks are\n",
    "+ An Overview of Machine Learning\n",
    "+ The Lumigator Platform üêä\n",
    "+ How does a Machine Learning evaluation workflows looks like\n",
    "+ The Thunderbird Ground Truth Dataset\n",
    "+ Selecting models to perform summarization:\n",
    "  + Using one encoder/decoder (BART)\n",
    "  + Utilizing two decoders (Mistral and GPT-4) to evaluate against ground truth\n",
    "+ Running evaluation experiments\n",
    "+ Discussing results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e41dab1",
   "metadata": {},
   "source": [
    "## Jupyter Walkthrough\n",
    "\n",
    "[Jupyter Notebooks](https://jupyter-notebook.readthedocs.io/en/stable/) provide an executable environment for running (usually) Python code alongside text. To work with Jupyter, click \"Run Cell\" to execute the code and view the results below the cell you are currently running. Cells are executed sequentially and can contain either text (Markdown) or Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf10cbb-f6a0-4a24-9d6d-30fac1f7d996",
   "metadata": {},
   "source": [
    "### Running cells \n",
    "To run a cell, press the \"play\" icon in the top bar (you can also hit Shift+Enter to run and proceed to the following cell).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mozilla-ai/lumigator/main/docs/assets/running.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "Your files are located on the left-hand side. They'll be saved for the duration of our session, but if you'd like to keep them, make sure to download them. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mozilla-ai/lumigator/main/docs/assets/files.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d284df-b55c-43ea-af33-6909392baa0f",
   "metadata": {},
   "source": [
    "Lets' try running some code! Execute the following code and verify the output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb7563e-3ab3-486f-b45a-7bd0c47fbcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Welcome to Lumigator!üêä\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecaa846",
   "metadata": {},
   "source": [
    "## Machine learning glossary\n",
    "\n",
    "Some terms you'll hear us using throughout the session: \n",
    "\n",
    "+ **Machine learning** - The process of creating a model that learns from data\n",
    "+ **Dataset** - Data used to train models and evaluate their performance\n",
    "+ **LLM** - Large language model, [a text-based model that performs next-word predictions](https://www.nvidia.com/en-us/glossary/large-language-models/) \n",
    "+ **Tokens** - Words broken up into pieces to be used in an LLM \n",
    "+ **Inference** - The process of getting a prediction from a large language model \n",
    "+ **Embeddings** - Numerical representations of text generated by modeling \n",
    "+ **Encoder-decoder models** - A neural network architecture comprised of two neural networks, an encoder that takes the input vectors from our data and creates an embedding of a fixed length, and a decoder, also a neural network, which takes the embeddings encoded as input and generates a static set of outputs such as translated text or a text summary\n",
    "+ **Decoder-only models** - Given a fixed input prompt, uses its representation to generate a sequence of words one at a time, with each word being conditioned on the ones generated previously\n",
    "+ **Task** - Machine learning tasks to fit a specific model type, including translation, summarization, completion, etc. \n",
    "+ **Ground truth** - Information that has been evaluated to be true by humans (or LLMs, in some cases), that we can use to evaluate and compare trained models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709af016-5f3f-4552-8a8a-e731e60eb89d",
   "metadata": {},
   "source": [
    "The process of **machine learning** is the process of creating a mathematical model that tries to approximate the world. A **machine learning model** is a set of instructions for generating a given\n",
    "output from data. The instructions are learned from the features of the input data itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545e65e6-4e05-4aac-9bd5-c8521fbcb9e2",
   "metadata": {},
   "source": [
    "Within the universe of modeling approaches, there are **supervised** and **unsupervised** approaches, as well as reinforcement learning. \n",
    "Language modeling of the kind we do with LLMs falls in the realm of neural network approaches. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mozilla-ai/lumigator/main/docs/assets/ml_family.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f67ff2a",
   "metadata": {},
   "source": [
    "### How do LLMs work? \n",
    "\n",
    "There are many different kinds of LLMs and many different kinds of architectures. For our evaluations, we use two different kinds:\n",
    "\n",
    "+ **Encoder/Decoder** - BART is an encoder/decoder model that converts input data into a fixed-size representation (similar to encoder models). These models are trained first to transform text into numerical representations, then to output text based on those numerical representations. They're good for synthesis as opposed to generation. \n",
    "+ **Decoder-only** - most GPT-family models, like Mistral, GPT, and others we'll be working with, are pre-trained with text data in an autoregressive manner, for next-token prediction given previous tokens.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8139990",
   "metadata": {},
   "source": [
    "### LLM Evaluation Workflows\n",
    "\n",
    "The following steps outline the phases that an LLM goes through during evaluation:\n",
    "\n",
    "1. Generate ground truth for our business use-case\n",
    "1. Pick several models we'd like to use to evaluate\n",
    "1. Run an evaluation loop consisting of looking at the ground truth in comparison to model output\n",
    "1. Analyze the evaluation results\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mozilla-ai/lumigator/main/docs/assets/llm_steps.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c44d8a1",
   "metadata": {},
   "source": [
    "Lumigator on a technical level is a **Python-based FastAPI web app** with services that run **jobs and deployments on a Ray cluster**, which can be run either locally or in the cloud, depending on your computer specs. Results and job metadata are stored in an SQL database. Larger models loaded from [HuggingFace](https://huggingface.co/) require GPUs.  \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mozilla-ai/lumigator/main/docs/assets/platform.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed38e49",
   "metadata": {},
   "source": [
    "What is Ray? [A distributed runtime for Python programs](https://github.com/ray-project/ray) that includes a Core library with primitives (Tasks, Actors, and objects) and a suite of ML libraries (Tune, Serve) that allow to build components of the machine learning model workflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838d2fd4",
   "metadata": {},
   "source": [
    "### Nota bene: Machine learning is alchemy\n",
    "\n",
    "When we think of traditional software application workflows, we think of an example such as adding a button. We can clearly test that we've added a blue button to our application, and that it works correctly. Machine learning is not like this! It involves a lot of experimentation, tweaking of hyperparameters and prompts and trying different models. Expect for the process to be imperfect, with many iterative loops. Luckily, Lumigator helps take away the uncertainty of at least model selection. üôÇ\n",
    "\n",
    "> There‚Äôs a self-congratulatory feeling in the air. We say things like ‚Äúmachine learning is the new electricity‚Äù. I‚Äôd like to offer an alternative metaphor: machine learning has become alchemy. - [Ben Recht and Ali Rahimi](https://archives.argmin.net/2017/12/05/kitchen-sinks/)\n",
    "\n",
    "Ultimately, the final conclusion of whether a model is good is if humans think it's good. \n",
    "\n",
    "With that in mind, let's dive into setting up experiments with Lumigator to test our models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7afa9ec9b5f866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages we need to work with data \n",
    "# python standard libraries\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Random string generator\n",
    "import random\n",
    "import string\n",
    "import shortuuid\n",
    "\n",
    "# third-party libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sdk.lumigator import LumigatorClient\n",
    "from schemas.jobs import JobType, JobCreate\n",
    "from schemas.datasets import DatasetFormat\n",
    "\n",
    "# wrap columns for inspection\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "# stylesheet for visibility\n",
    "plt.style.use(\"fast\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1cd443f885af1d",
   "metadata": {},
   "source": [
    "# Understanding the Lumigator App and API \n",
    "\n",
    "The app itself consists of an API, which you can access and test out methods in the [OpenAPI spec](https://swagger.io/specification/), at the platform URL, under docs.\n",
    "If you are running Lumigator as a local installation, you can directly access the API at [this URL](http://localhost:8000/docs).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mozilla-ai/lumigator/main/docs/assets/openapi.png\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd6f7aa716833f7",
   "metadata": {},
   "source": [
    "Large language models today are consumed in one of several ways:\n",
    "\n",
    "+ As **API endpoints** for proprietary models hosted by [OpenAI](https://openai.com/), [Anthropic](https://www.anthropic.com/), or major cloud providers\n",
    "+ As **model artifacts** downloaded from HuggingFace‚Äôs Model Hub and/or trained/fine-tuned using HuggingFace libraries and hosted on local storage\n",
    "+ As model artifacts available in a format optimized for **local inference**, typically [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md), and accessed via applications like [llama.cpp](https://github.com/ggerganov/llama.cpp) or [ollama](https://ollama.com/)\n",
    "+ As [ONNX](https://onnx.ai/), a format which optimizes sharing between backend ML frameworks\n",
    "\n",
    "We use API endpoints and local storage in Lumigator. We currently have 5 key endpoints on the platform:\n",
    "\n",
    "+ `Health` - Status of the application, running status of jobs and deployments. \n",
    "+ `Datasets` - Data that we add to the platform for evaluation. We can upload, delete, and save different data in the platform. - We'll use this to save our ground truth and experiment data\n",
    "+ `Jobs` - Our actual evaluation experiments. We can list all previous evaluations, create new ones, and get their results.\n",
    "+ `Completions` - Access to external APIs such as Mistral and OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a28c048ea00706",
   "metadata": {},
   "source": [
    "## Model Task: Summarization\n",
    "\n",
    "The task we'll be working with is **summarization**, aka we want to generate a summary of our text. \n",
    "\n",
    "In our business case, which is to create summaries of conversation threads, much as you might see in Slack or an email chain, the models need to be able to extract key information from those threads while still being able to accept a large context window to capture the entire conversation history. \n",
    "\n",
    "We identified that it is far more valuable to conduct **abstractive** summaries‚Äîsummaries that identify important sections in the text and generate highlights‚Äîrather than **extractive** ones, which select a subset of sentences and staple them together. This is because the final interface will be in natural language, and we want to avoid summaries that are interpreted from often incoherent text snippets produced by extractive methods.\n",
    "\n",
    "For more on summarization as a use-case, [see our blog post here.](https://blog.mozilla.ai/on-model-selection-for-text-summarization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33e80b64bad99e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Ground Truth for Models\n",
    "\n",
    "The term ground truth comes from geology and geospatial sciences, where actual information was collected on the ground to validate data acquired through remote sensing, such as satellite imagery or aerial photography. Since then, the concept has been adopted in other fields, particularly in machine learning and artificial intelligence, to refer to the accurate, real-world data used for training and testing models. \n",
    "\n",
    "The **best ground truth is human-generated** but building it is a very expensive task. One recent trend is to rely on large language models but (as you will see later) they have their own pitfalls. An intermediate approach uses different LLMs to provide ground truth \"candidates\" which are then subject to human pairwise evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d102a0b4515342",
   "metadata": {},
   "source": [
    "## Our Input data\n",
    "\n",
    "The data we'll be using in this walkthrough comes from [DialogSum](https://github.com/cylnlp/DialogSum), a large-scale labeled dialogue summarization dataset which comes with ground truth provided by human annotators.\n",
    "\n",
    "For the sake of explanation, in the next section we will show how Lumigator allows one to generate a ground truth candidates with the help of a language model. However, in the following evaluation section we will compare against the original, human-provided, annotations.\n",
    "\n",
    "Here follows a brief description of DialogSum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65af6945-bd5d-4506-af3c-144683779d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset is available at https://huggingface.co/datasets/knkarthick/dialogsum\n",
    "# and can be directly downloaded with the `load_dataset` method\n",
    "dataset = 'knkarthick/dialogsum'\n",
    "ds = load_dataset(dataset, split='validation')\n",
    "df = ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb05409-aad2-42e2-bc23-743575ad924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine a single sample \n",
    "df['dialogue'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac02084ba6b6fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a function to do some simple character counts for model input\n",
    "df['char_count'] = df['dialogue'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90ceb9398ae3ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect our data\n",
    "df.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359677f7f12bcefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show statistics about characters count\n",
    "df['char_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116e1127c3846883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate plot of character counts\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.hist(df['char_count'], bins=30)\n",
    "ax.set_xlabel('Character Count')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "stats = df['char_count'].describe().apply(lambda x: f\"{x:.0f}\")\n",
    "\n",
    "# Add text boxes for statistics\n",
    "plt.text(1.05, 0.95, stats.to_string(), \n",
    "         transform=ax.transAxes, verticalalignment='top')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(right=0.75)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc4643b-d6b4-40e8-97d1-c6b9cb50d7ae",
   "metadata": {},
   "source": [
    "## Ground Truth Generation with Mistral\n",
    "\n",
    "In the following we will use [Mistral API](https://docs.mistral.ai/api/) to generate candidate ground truth. Note that for the following code to work, you need to have a valid mistral API key (no worries if you don't, the next section shows another methods which relies on an opensource model running locally on your computer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba1b43d-52d4-455c-8d79-4507ea5ec58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_client = LumigatorClient(f\"{os.getenv('LUMIGATOR_SERVICE_HOST', 'localhost')}:8000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f676203834ab1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Ground Truth Generation with Mistral\n",
    "vendor = \"mistral\"\n",
    "mistral_responses = []\n",
    "\n",
    "for sample in df['dialogue'][0:10]:\n",
    "    res = lm_client.completions.get_completion(vendor, sample)\n",
    "    print(f\"Mistral Summary:\", res)\n",
    "    mistral_responses.append((sample, res.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0528d0e-ffd9-422d-ac54-9d49403b7e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a result set we can look at\n",
    "mistral_results_df = pd.DataFrame(mistral_responses, columns=['examples', 'mistral_response'])\n",
    "mistral_results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378251f3-4145-4fa6-b933-564a91101c44",
   "metadata": {},
   "source": [
    "## Save and upload datasets\n",
    "\n",
    "Now that you‚Äôve seen how to generate ground truth using an LLM, let‚Äôs save the dataset and make it available to Lumigator for further experiments. For each example (the `dialogsum` original dataset and the `mistral-generated` ground truth), we will perform the following operations:\n",
    "\n",
    "1. Make sure that the two main fields (original text and ground truth) are called `examples` and `ground_truth`, which are the names internally used by Lumigator to refer to them, and save the datasets as CSV files.\n",
    "1. Make the dataset available to Lumigator with the `create_dataset` method provided by the Lumigator SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0f300c-9651-473b-aca9-1f147a3f2f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.remove_columns([\"id\", \"topic\"])\n",
    "ds = ds.rename_column(\"dialogue\", \"examples\")\n",
    "ds = ds.rename_column(\"summary\", \"ground_truth\")\n",
    "\n",
    "dataset_name = \"dialogsum_converted.csv\"\n",
    "ds.to_csv(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ec30ff-f639-4ab4-b97d-eadbe3df8075",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_client.datasets.create_dataset(\n",
    "    open(dataset_name, \"rb\"),\n",
    "    DatasetFormat.JOB\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4115d37099e35f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_results_df = mistral_results_df.rename(columns={\"mistral_response\": \"ground_truth\"})\n",
    "mistral_results_df.to_csv('mistral_ground_truth.csv', index=False)\n",
    "\n",
    "dataset_name = \"mistral_ground_truth.csv\"\n",
    "\n",
    "lm_client.datasets.create_dataset(\n",
    "    open(dataset_name, \"rb\"),\n",
    "    DatasetFormat.JOB\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e5b2701933bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And let's check that data loaded\n",
    "datasets = lm_client.datasets.get_datasets()\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3019b45b211087",
   "metadata": {},
   "source": [
    "## Jobs\n",
    "\n",
    "After generating the ground truth (either manually or with the aid of some models) and uploading the dataset to lumigator, we are ready to start evaluating models on it. Note that when you uploaded your datasets you got back some information that included a dataset `id`. This is a unique identifier to your own dataset that you can reuse across different jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98f8a6fdff48654",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = datasets.items[0].id\n",
    "\n",
    "# now look for the dataset on lumigator\n",
    "result = lm_client.datasets.get_dataset(dataset_id)\n",
    "dataset_id, dataset_name = result.id, result.filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51eb04dbc16b035",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "What you see below are different lists of models we have already tested for the summarization task.\n",
    "The `models` variable at the end provides you with a selection, but you can choose any combination of them:\n",
    "the default is a single local model (`facebook/bart-large-cnn`), but depending on your setup you can choose\n",
    "more and/or add different APIs.\n",
    "\n",
    "Note that different model types are specified with different prefixes:\n",
    "\n",
    "- `hf://` is used for HuggingFace models which are downloaded and ran as Ray jobs\n",
    "- `mistral://` is used for models which are accessed through the Mistral API\n",
    "- `oai://` is used for models which are accessed through an OpenAI-compatible API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f8c76c37a2b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here follows a list of models we have tested for summarization:\n",
    "# feel free to add any of them in the \"models\" list below\n",
    "#\n",
    "# Encoder-Decoder models\n",
    "#    'hf://facebook/bart-large-cnn',\n",
    "#    'hf://mikeadimech/longformer-qmsum-meeting-summarization', \n",
    "#    'hf://mrm8488/t5-base-finetuned-summarize-news',\n",
    "#    'hf://Falconsai/text_summarization',\n",
    "#\n",
    "# Decoder models\n",
    "#    'mistral://open-mistral-7b',\n",
    "#\n",
    "# GPTs\n",
    "#    \"oai://gpt-4o-mini\",\n",
    "#    \"oai://gpt-4-turbo\",\n",
    "#    \"oai://gpt-3.5-turbo-0125\",\n",
    "#\n",
    "models = [\n",
    "    'hf://facebook/bart-large-cnn',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa55eee63016041",
   "metadata": {},
   "source": [
    "## Run Evaluations\n",
    "\n",
    "The following cell will start the actual model evaluations.\n",
    "Once you run it, new jobs will be submitted to ray (one for each model) and the outcomes of these submissions will be printed.\n",
    "Each evaluation job will first use the provided model to summarize each of the emails in the input dataset. After that, it will calculate a few metrics to evaluate how close the predicted summaries are to the ground truth provided in the dataset.\n",
    "\n",
    "Each job starts with a `created` status. While the job runs, you will be able to track its status by running the cell in the section **Track evaluation jobs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f344b6c4998e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this value to limit the evaluation to the first max_samples items (0=all)\n",
    "max_samples = 10\n",
    "# team_name is a way to group jobs together under the same namespace, feel free to customize it\n",
    "team_name = \"lumigator_enthusiasts\"\n",
    "\n",
    "responses = []\n",
    "for model in models:\n",
    "    job_args = JobCreate(\n",
    "        name=team_name,\n",
    "        description=\"Test\",\n",
    "        model=model,\n",
    "        dataset=str(dataset_id),\n",
    "        max_samples=max_samples\n",
    "    )\n",
    "    # descr = f\"Testing {model} summarization model on {dataset_name}\"\n",
    "    responses.append(lm_client.jobs.create_job(JobType.EVALUATION, job_args))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f787190-4581-4c26-beca-0260303a68bd",
   "metadata": {},
   "source": [
    "### Track evaluation jobs\n",
    "\n",
    "Run the following to track your evaluation jobs.\n",
    "\n",
    "- *NOTE*: you won't be able to run other cells while this one is running. However, you can interrupt it whenever you want by clicking on the \"stop\" button above and run it at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89132dad-0bb7-4203-ba1b-f57394071ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = responses[0].id\n",
    "\n",
    "job = lm_client.jobs.wait_for_job(job_id)  # Create the coroutine object\n",
    "result = await job         # Await the coroutine to get the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d8cbf3-e44f-4f1e-b520-3f6fac147fca",
   "metadata": {},
   "source": [
    "## Show evaluation results\n",
    "\n",
    "Once all evaluations are completed, their results will be stored on our platform and available for download. You can download them individually using the `get_job_result()` method.\n",
    "\n",
    "The following cell iterates on all your job ids, downloads results from each, and builds a table comparing different metrics for each model.\n",
    "The metrics we use to evaluate are ROUGE, METEOR, and BERT score. They all measure similarity between predicted summaries and those provided with the ground truth, but each of them focuses on different aspects. The image below shows their main characteristics and the tradeoffs between their flexibility and their computational cost.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mozilla-ai/lumigator/main/docs/assets/metrics.png\" alt=\"drawing\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6596a8-bd73-4298-a2e5-604ce537bcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_ids = [job.id for job in lm_client.jobs.get_jobs().items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8709b8-59ba-4a9c-81bd-59b2fc612f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after the jobs complete, gather evaluation results\n",
    "eval_results = []\n",
    "for job_id in job_ids:\n",
    "    eval_results.append(lm_client.jobs.get_job_download(job_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4b0abd-ec25-4be3-9b75-57ce732e9fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results[0].download_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d4c7e6-183b-42b3-bd43-b55ad8024414",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "for eval_result in eval_results:\n",
    "    response = requests.request(url=eval_result.download_url, method=\"GET\")\n",
    "    responses.append(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b450ce",
   "metadata": {},
   "source": [
    "## Analysis of Evaluation Results\n",
    "\n",
    "The tablel above is just a summary of all the evaluation results.\n",
    "The `eval_results` object contains way more details from which you'll be able to get a few more insights in the following cells.\n",
    "\n",
    "### Direct access to all data\n",
    "\n",
    "The following cell shows you the kind of information that's available in each of the `eval_results` elements. This information is nested at different depth levels. You can access each using the `get_nested_value` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadbdaec-a1e9-4731-bfdd-b471e6a0fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_results is a list holding information for each of the models you defined before\n",
    "# for each element, you can access different metrics, time performance, and predictions\n",
    "eval_results[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec4d37e-4750-404e-b40c-3778b8196d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how much time it took for a model to summarize all the input samples\n",
    "ld.get_nested_value(eval_results[0], \"summarization_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a73261-e5a4-46a6-9b7c-1bc5e62dc1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see all the bertscore data\n",
    "ld.get_nested_value(eval_results[0], \"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6e8285-f407-4a5f-acda-3400554c3d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see mean bert precision\n",
    "ld.get_nested_value(eval_results[0], \"bertscore/precision_mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133c21f4-e61a-4f07-975f-052ccdead1da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
